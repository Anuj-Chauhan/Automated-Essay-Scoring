{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from textstat.textstat import textstatistics, easy_word_set, legacy_round \n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('/Users/anuj/Datasets/data_mining/asap-aes/training_set_rel3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>rater1_trait1</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12978.000000</td>\n",
       "      <td>12978.000000</td>\n",
       "      <td>12977.000000</td>\n",
       "      <td>12977.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>12977.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>2292.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2292.000000</td>\n",
       "      <td>2292.000000</td>\n",
       "      <td>723.000000</td>\n",
       "      <td>723.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10295.432809</td>\n",
       "      <td>4.179458</td>\n",
       "      <td>4.126840</td>\n",
       "      <td>4.137089</td>\n",
       "      <td>37.828125</td>\n",
       "      <td>6.799723</td>\n",
       "      <td>3.333889</td>\n",
       "      <td>3.330556</td>\n",
       "      <td>3.333889</td>\n",
       "      <td>2.444154</td>\n",
       "      <td>...</td>\n",
       "      <td>2.635689</td>\n",
       "      <td>2.710297</td>\n",
       "      <td>3.777317</td>\n",
       "      <td>3.589212</td>\n",
       "      <td>3.945312</td>\n",
       "      <td>3.890625</td>\n",
       "      <td>4.078125</td>\n",
       "      <td>3.992188</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>3.617188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6308.588616</td>\n",
       "      <td>2.136749</td>\n",
       "      <td>4.212537</td>\n",
       "      <td>4.264320</td>\n",
       "      <td>5.240829</td>\n",
       "      <td>8.970558</td>\n",
       "      <td>0.729103</td>\n",
       "      <td>0.726807</td>\n",
       "      <td>0.729103</td>\n",
       "      <td>1.211730</td>\n",
       "      <td>...</td>\n",
       "      <td>1.142566</td>\n",
       "      <td>1.045795</td>\n",
       "      <td>0.689401</td>\n",
       "      <td>0.693256</td>\n",
       "      <td>0.643668</td>\n",
       "      <td>0.630390</td>\n",
       "      <td>0.622535</td>\n",
       "      <td>0.509687</td>\n",
       "      <td>0.538845</td>\n",
       "      <td>0.603417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4439.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10045.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15680.750000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21633.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           essay_id     essay_set  rater1_domain1  rater2_domain1  \\\n",
       "count  12978.000000  12978.000000    12977.000000    12977.000000   \n",
       "mean   10295.432809      4.179458        4.126840        4.137089   \n",
       "std     6308.588616      2.136749        4.212537        4.264320   \n",
       "min        1.000000      1.000000        0.000000        0.000000   \n",
       "25%     4439.250000      2.000000        2.000000        2.000000   \n",
       "50%    10045.500000      4.000000        3.000000        3.000000   \n",
       "75%    15680.750000      6.000000        4.000000        4.000000   \n",
       "max    21633.000000      8.000000       30.000000       30.000000   \n",
       "\n",
       "       rater3_domain1  domain1_score  rater1_domain2  rater2_domain2  \\\n",
       "count      128.000000   12977.000000     1800.000000     1800.000000   \n",
       "mean        37.828125       6.799723        3.333889        3.330556   \n",
       "std          5.240829       8.970558        0.729103        0.726807   \n",
       "min         20.000000       0.000000        1.000000        1.000000   \n",
       "25%         36.000000       2.000000        3.000000        3.000000   \n",
       "50%         40.000000       3.000000        3.000000        3.000000   \n",
       "75%         40.000000       8.000000        4.000000        4.000000   \n",
       "max         50.000000      60.000000        4.000000        4.000000   \n",
       "\n",
       "       domain2_score  rater1_trait1  ...  rater2_trait3  rater2_trait4  \\\n",
       "count    1800.000000    2292.000000  ...    2292.000000    2292.000000   \n",
       "mean        3.333889       2.444154  ...       2.635689       2.710297   \n",
       "std         0.729103       1.211730  ...       1.142566       1.045795   \n",
       "min         1.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         3.000000       2.000000  ...       2.000000       2.000000   \n",
       "50%         3.000000       2.000000  ...       2.000000       3.000000   \n",
       "75%         4.000000       3.000000  ...       4.000000       3.000000   \n",
       "max         4.000000       6.000000  ...       6.000000       6.000000   \n",
       "\n",
       "       rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
       "count     723.000000     723.000000     128.000000     128.000000   \n",
       "mean        3.777317       3.589212       3.945312       3.890625   \n",
       "std         0.689401       0.693256       0.643668       0.630390   \n",
       "min         1.000000       1.000000       2.000000       2.000000   \n",
       "25%         3.000000       3.000000       4.000000       4.000000   \n",
       "50%         4.000000       4.000000       4.000000       4.000000   \n",
       "75%         4.000000       4.000000       4.000000       4.000000   \n",
       "max         6.000000       6.000000       6.000000       6.000000   \n",
       "\n",
       "       rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "count     128.000000     128.000000     128.000000     128.000000  \n",
       "mean        4.078125       3.992188       3.843750       3.617188  \n",
       "std         0.622535       0.509687       0.538845       0.603417  \n",
       "min         2.000000       3.000000       2.000000       2.000000  \n",
       "25%         4.000000       4.000000       4.000000       3.000000  \n",
       "50%         4.000000       4.000000       4.000000       4.000000  \n",
       "75%         4.000000       4.000000       4.000000       4.000000  \n",
       "max         6.000000       6.000000       5.000000       5.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('essay_set').agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set1 = data[data['essay_set'] == 1]\n",
    "data_set3 = data[data['essay_set'] == 3]\n",
    "data_set5 = data[data['essay_set'] == 5]\n",
    "data_set6 = data[data['essay_set'] == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I think that computers have a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Did you know that more and more people these d...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>@PERCENT1 of people agree that computers make ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear reader, @ORGANIZATION1 has had a dramatic...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>In the @LOCATION1 we have the technology of a ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "5         6          1  Dear @LOCATION1, I think that computers have a...   \n",
       "6         7          1  Did you know that more and more people these d...   \n",
       "7         8          1  @PERCENT1 of people agree that computers make ...   \n",
       "8         9          1  Dear reader, @ORGANIZATION1 has had a dramatic...   \n",
       "9        10          1  In the @LOCATION1 we have the technology of a ...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  domain1_score  \n",
       "0             4.0             4.0            8.0  \n",
       "1             5.0             4.0            9.0  \n",
       "2             4.0             3.0            7.0  \n",
       "3             5.0             5.0           10.0  \n",
       "4             4.0             4.0            8.0  \n",
       "5             4.0             4.0            8.0  \n",
       "6             5.0             5.0           10.0  \n",
       "7             5.0             5.0           10.0  \n",
       "8             4.0             5.0            9.0  \n",
       "9             5.0             4.0            9.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set1.head(10).dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dear @CAPS1 @CAPS2, I believe that using computers will benefit us in many ways like talking and becoming friends will others through websites like facebook and mysace. Using computers can help us find coordibates, locations, and able ourselfs to millions of information. Also computers will benefit us by helping with jobs as in planning a house plan and typing a @NUM1 page report for one of our jobs in less than writing it. Now lets go into the wonder world of technology. Using a computer will help us in life by talking or making friends on line. Many people have myspace, facebooks, aim, these all benefit us by having conversations with one another. Many people believe computers are bad but how can you make friends if you can never talk to them? I am very fortunate for having a computer that can help with not only school work but my social life and how I make friends. Computers help us with finding our locations, coordibates and millions of information online. If we didn't go on the internet a lot we wouldn't know how to go onto websites that @MONTH1 help us with locations and coordinates like @LOCATION1. Would you rather use a computer or be in @LOCATION3. When your supposed to be vacationing in @LOCATION2. Million of information is found on the internet. You can as almost every question and a computer will have it. Would you rather easily draw up a house plan on the computers or take @NUM1 hours doing one by hand with ugly erazer marks all over it, you are garrenteed that to find a job with a drawing like that. Also when appling for a job many workers must write very long papers like a @NUM3 word essay on why this job fits you the most, and many people I know don't like writing @NUM3 words non-stopp for hours when it could take them I hav an a computer. That is why computers we needed a lot now adays. I hope this essay has impacted your descion on computers because they are great machines to work with. The other day I showed my mom how to use a computer and she said it was the greatest invention sense sliced bread! Now go out and buy a computer to help you chat online with friends, find locations and millions of information on one click of the button and help your self with getting a job with neat, prepared, printed work that your boss will love.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set1.essay[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set1.dropna(axis = 1, inplace=True)\n",
    "data_set3.dropna(axis = 1, inplace=True)\n",
    "data_set5.dropna(axis = 1, inplace=True)\n",
    "data_set6.dropna(axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1165"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validate if domain1_score is simply the sum of 2 scorers -- True\n",
    "data_set1[data_set1['rater1_domain1'] + data_set1['rater2_domain1'] != data_set1['domain1_score']]\n",
    "\n",
    "#Number of cases where the rater1 and rater2 scores don't match -- 618\n",
    "len(data_set1[data_set1['rater1_domain1'] != data_set1['rater2_domain1']])\n",
    "\n",
    "#Number of cases where rater1 and rater2 scores match -- 1165\n",
    "len(data_set1) - len(data_set1[data_set1['rater1_domain1'] != data_set1['rater2_domain1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     kNN using word vectors (find the most similar documents and get a weighted score)\n",
    "#     Simple linear regression using word counts, sentence length, number of distinct words and # verbs/nouns (as well as ratios/percentages of the pairs)\n",
    "#     Boosted decision trees on the same features as above.\n",
    "#     Multiclass SVM trained on the word vectors using the score as the \"class\"\n",
    "#     Support vector regression trained on the word vectors using the score as target.\n",
    "#     Singular value decomposition on the word vectors.\n",
    "#     Linear combinations of all the above.\n",
    "\n",
    "# Results\n",
    "# Global parameters alone got me to around 0.71, \n",
    "# adding kNN got me to 0.74. \n",
    "# with the SVMs since I couldn't get past 0.75 with these features/algos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_set = data_set1[['essay_id', 'essay', 'domain1_score']]\n",
    "work_set_3 = data_set3[['essay_id', 'essay', 'domain1_score']]\n",
    "work_set_5 = data_set5[['essay_id', 'essay', 'domain1_score']]\n",
    "work_set_6 = data_set6[['essay_id', 'essay', 'domain1_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set1[['essay', 'domain1_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addSentenceLength (row):\n",
    "    return len(re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnSentences(text):\n",
    "    return re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNounCount (row):\n",
    "    count = 0\n",
    "    for sentence in row.split('. '):\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            if (token.pos_ == 'NOUN'):\n",
    "                count += 1\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPropnCount (row):\n",
    "    count = 0\n",
    "    \n",
    "    for sentence in row.split('. '):\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            if (token.pos_ == 'PROPN'):\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addVerbCount (row):\n",
    "    count = 0\n",
    "    \n",
    "    for sentence in row.split('. '):\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            if (token.pos_ == 'VERB'):\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addAdjCount (row):\n",
    "    count = 0\n",
    "    \n",
    "    for sentence in row.split('. '):\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            if (token.pos_ == 'ADJ'):\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWordCount (row):\n",
    "    return len(row.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDistinctWords (row):\n",
    "    unstop = {word for word in row.split() if word not in nlp.Defaults.stop_words}\n",
    "    return len(unstop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addReadabilityIndex (row):\n",
    "    charCount = len(row.replace(\" \", \"\"))\n",
    "    wordCount = len(row.split())\n",
    "    senCount = len(re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', row))\n",
    "    \n",
    "    return (4.71 * (charCount/wordCount) + 0.5 * (wordCount/senCount) - 21.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllableCount(row):\n",
    "    row = row.lower()\n",
    "    count = 0\n",
    "    for word in row.split():\n",
    "        vowels = \"aeiouy\"\n",
    "        if word[0] in vowels:\n",
    "            count += 1\n",
    "        for index in range(1, len(word)):\n",
    "            if word[index] in vowels and word[index - 1] not in vowels:\n",
    "                count += 1\n",
    "        if word.endswith(\"e\"):\n",
    "            count -= 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllables_count(word): \n",
    "    return textstatistics().syllable_count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_syllables_per_word(text): \n",
    "    syllable = syllables_count(text) \n",
    "    words = addWordCount(text) \n",
    "    ASPW = float(syllable) / float(words) \n",
    "    return legacy_round(ASPW,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_length(text): \n",
    "    words = addWordCount(text) \n",
    "    sentences = addSentenceLength(text) \n",
    "    average_sentence_length = float(words / sentences) \n",
    "    return average_sentence_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difficult_words(text): \n",
    "  \n",
    "    # Find all words in the text \n",
    "    words = [] \n",
    "    sentences = returnSentences(text)\n",
    "    for sentence in sentences: \n",
    "        words += [str(token) for token in sentence] \n",
    "  \n",
    "    # difficult words are those with syllables >= 2 \n",
    "    # easy_word_set is provide by Textstat as  \n",
    "    # a list of common words \n",
    "    diff_words_set = set() \n",
    "      \n",
    "    for word in words: \n",
    "        syllable_count = syllables_count(word) \n",
    "        if word not in easy_word_set and syllable_count >= 2: \n",
    "            diff_words_set.add(word) \n",
    "  \n",
    "    return len(diff_words_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_syllable_count(text): \n",
    "    count = 0\n",
    "    words = [] \n",
    "    sentences = returnSentences(text) \n",
    "    for sentence in sentences: \n",
    "        words += [token for token in sentence] \n",
    "      \n",
    "  \n",
    "    for word in words: \n",
    "        syllable_count = syllables_count(word) \n",
    "        if syllable_count >= 3: \n",
    "            count += 1\n",
    "    return count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flesch_reading_ease(text): \n",
    "    \"\"\" \n",
    "        Implements Flesch Formula: \n",
    "        Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW) \n",
    "        Here, \n",
    "          ASL = average sentence length (number of words  \n",
    "                divided by number of sentences) \n",
    "          ASW = average word length in syllables (number of syllables  \n",
    "                divided by number of words) \n",
    "    \"\"\"\n",
    "    FRE = 206.835 - float(1.015 * avg_sentence_length(text)) - float(84.6 * avg_syllables_per_word(text)) \n",
    "        \n",
    "    return legacy_round(FRE, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gunning_fog(text): \n",
    "    per_diff_words = (difficult_words(text) / addWordCount(text) * 100) + 5\n",
    "    grade = 0.4 * (avg_sentence_length(text) + per_diff_words) \n",
    "    return grade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smog_index(text): \n",
    "    \"\"\" \n",
    "        Implements SMOG Formula / Grading \n",
    "        SMOG grading = 3 + ?polysyllable count. \n",
    "        Here,  \n",
    "           polysyllable count = number of words of more \n",
    "          than two syllables in a sample of 30 sentences. \n",
    "    \"\"\"\n",
    "  \n",
    "    if addSentenceLength(text) >= 3: \n",
    "        poly_syllab = poly_syllable_count(text) \n",
    "        \n",
    "        SMOG = (1.043 * (30*(poly_syllab / addSentenceLength(text)))**0.5) + 3.1291\n",
    "        \n",
    "        return legacy_round(SMOG, 3) \n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dale_chall_readability_score(text): \n",
    "    \"\"\" \n",
    "        Implements Dale Challe Formula: \n",
    "        Raw score = 0.1579*(PDW) + 0.0496*(ASL) + 3.6365 \n",
    "        Here, \n",
    "            PDW = Percentage of difficult words. \n",
    "            ASL = Average sentence length \n",
    "    \"\"\"\n",
    "    words = addWordCount(text) \n",
    "    # Number of words not termed as difficult words \n",
    "    count = words - difficult_words(text) \n",
    "    if words > 0: \n",
    "  \n",
    "        # Percentage of words not on difficult word list \n",
    "  \n",
    "        per = float(count) / float(words) * 100\n",
    "      \n",
    "    # diff_words stores percentage of difficult words \n",
    "    diff_words = 100 - per \n",
    "  \n",
    "    raw_score = (0.1579 * diff_words) + (0.0496 * avg_sentence_length(text)) \n",
    "      \n",
    "    # If Percentage of Difficult Words is greater than 5 %, then; \n",
    "    # Adjusted Score = Raw Score + 3.6365, \n",
    "    # otherwise Adjusted Score = Raw Score \n",
    "  \n",
    "    if diff_words > 5:        \n",
    "  \n",
    "        raw_score += 3.6365\n",
    "          \n",
    "    return legacy_round(raw_score, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flesch_reading_ease\n",
    "# gunning_fog\n",
    "# smog_index\n",
    "# dale_chall_readability_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureExtractor(work_set):\n",
    "    \n",
    "    start = time.time()\n",
    "    end = time.time()\n",
    "    work_set['nounCount'] = work_set.essay.apply(addNounCount)\n",
    "    #end = time.time()\n",
    "    print(\"Noun count :\", time.time() - end)\n",
    "    \n",
    "    work_set['propnCount'] = work_set.essay.apply(addPropnCount)\n",
    "    print (\"PropN count :\", time.time() - end)\n",
    "    end = time.time()\n",
    "    \n",
    "    work_set['verbCount'] = work_set.essay.apply(addVerbCount)\n",
    "    print (\"Verb count :\", time.time() - end)\n",
    "    end = time.time()\n",
    "    \n",
    "    work_set['adjCount'] = work_set.essay.apply(addAdjCount)\n",
    "    print (\"ADJ count :\", time.time() - end)\n",
    "    end = time.time()\n",
    "    \n",
    "    work_set['senCount'] = work_set.essay.apply(addSentenceLength)\n",
    "    work_set['wordCount'] = work_set.essay.apply(addWordCount)\n",
    "    work_set['distinctCount'] = work_set.essay.apply(addDistinctWords)\n",
    "    work_set['syllableCount'] = work_set.essay.apply(syllableCount)\n",
    "    work_set['avgSPerWord'] = work_set.essay.apply(avg_syllables_per_word)\n",
    "    print (\"Numerical count :\", time.time() - end)\n",
    "    end = time.time()\n",
    "    \n",
    "    work_set['readabilityIndex'] = work_set.essay.apply(addReadabilityIndex)\n",
    "    print (\"SRI count :\", time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    work_set['riFRE'] = work_set.essay.apply(flesch_reading_ease)\n",
    "    print (\"FRE count :\", time.time() - end)\n",
    "    end = time.time()\n",
    "    \n",
    "    work_set['riGF'] = work_set.essay.apply(gunning_fog)\n",
    "    print (\"GF count :\", time.time() - end)\n",
    "    end = time.time()\n",
    "    \n",
    "    work_set['riSI'] = work_set.essay.apply(smog_index)\n",
    "    print (\"SI count :\", time.time() - end)\n",
    "    end = time.time()\n",
    "    \n",
    "    work_set['riDC'] = work_set.essay.apply(dale_chall_readability_score)\n",
    "    print (\"DC count :\", time.time() - end)\n",
    "    end = time.time()\n",
    "    \n",
    "    print (\"Finished feature extraction in:\", time.time() - start)\n",
    "    print ()\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureExtractor(work_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureExtractor(work_set_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureExtractor(work_set_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureExtractor(work_set_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work_set.to_pickle(\"./work_set_feats.pkl\")\n",
    "# work_set_3.to_pickle(\"./work_set_3_feats.pkl\")\n",
    "# work_set_5.to_pickle(\"./work_set_5_feats.pkl\")\n",
    "# work_set_6.to_pickle(\"./work_set_6_feats.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_set = pd.read_pickle(\"./work_set_feats.pkl\")\n",
    "work_set_3 = pd.read_pickle(\"./work_set_3_feats.pkl\")\n",
    "work_set_5 = pd.read_pickle(\"./work_set_5_feats.pkl\")\n",
    "work_set_6 = pd.read_pickle(\"./work_set_6_feats.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "work_set.drop(columns= ['essay_id', 'essay'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = work_set_6.drop(['domain1_score','essay','essay_id'], 1)\n",
    "y = work_set_6['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"R2 score on the Train:\\t{:0.3f}\".format(r2_score(y_train, classifier.predict(X_train))))\n",
    "print(\"R2 score on the Test:\\t{:0.3f}\".format(r2_score(y_test, classifier.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"R2 score on the Train:\\t{:0.3f}\".format(r2_score(y_train, knn.predict(X_train))))\n",
    "print(\"R2 score on the Test:\\t{:0.3f}\".format(r2_score(y_test, knn.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = work_set_3.drop(['domain1_score','essay','essay_id'], 1)\n",
    "y = work_set_3['domain1_score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = preprocessing.scale(X_train)\n",
    "scaled_y_train = preprocessing.scale(y_train)\n",
    "scaled_X_test = preprocessing.scale(X_test)\n",
    "scaled_y_test = preprocessing.scale(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(72, activation=tf.nn.relu, input_shape=[len(X_train.keys())]),\n",
    "        layers.Dense(64, activation=tf.nn.relu),\n",
    "        layers.Dense(48, activation=tf.nn.relu),\n",
    "        layers.Dense(1)\n",
    "      ])\n",
    "    \n",
    "    goldenValue = 0.001\n",
    "    optimizer = tf.keras.optimizers.RMSprop(goldenValue)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer='adam',\n",
    "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0: print('Processed {} epochs.'.format(epoch))\n",
    "\n",
    "EPOCHS = 40\n",
    "\n",
    "history = model.fit(scaled_X_train, scaled_y_train, epochs=EPOCHS+1, validation_split = 0.2, verbose=0,\n",
    "  callbacks=[PrintDot()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates and prints r2 score of training and testing data\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(scaled_y_train, model.predict(scaled_X_train))))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(scaled_y_test, model.predict(scaled_X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [Domain Score]')\n",
    "    plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "           label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "           label = 'Val Error')\n",
    "    plt.ylim([0,5])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error [$Domain Score^2$]')\n",
    "    plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
    "           label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
    "           label = 'Val Error')\n",
    "    plt.ylim([0,20])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(scaled_X_train, scaled_y_train, epochs=EPOCHS,\n",
    "                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(scaled_X_train).flatten()\n",
    "\n",
    "plt.scatter(scaled_y_train, test_predictions)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0,plt.xlim()[1]])\n",
    "plt.ylim([0,plt.ylim()[1]])\n",
    "_ = plt.plot([-100, 100], [-100, 100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_set = pd.read_pickle('./work_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMetaNeural(df, fname = \"metafile\"):\n",
    "    df.pop('essay')\n",
    "    columns = [key for key in df.keys() if (key  != 'essay_id') and (key != 'domain1_score')]\n",
    "    columns.append('domain1_score')\n",
    "    columns.append('essay_id')\n",
    "    \n",
    "    df[columns].to_csv(fname+'.csv', header=False, index=False)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id                                              essay  domain1_score\n",
       "0         1  Dear local newspaper, I think effects computer...            8.0\n",
       "1         2  Dear @CAPS1 @CAPS2, I believe that using compu...            9.0\n",
       "2         3  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...            7.0\n",
       "3         4  Dear Local Newspaper, @CAPS1 I have found that...           10.0\n",
       "4         5  Dear @LOCATION1, I know having computers has a...            8.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Dear local newspaper, I think effects computer...\n",
       "1       Dear @CAPS1 @CAPS2, I believe that using compu...\n",
       "2       Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...\n",
       "3       Dear Local Newspaper, @CAPS1 I have found that...\n",
       "4       Dear @LOCATION1, I know having computers has a...\n",
       "5       Dear @LOCATION1, I think that computers have a...\n",
       "6       Did you know that more and more people these d...\n",
       "7       @PERCENT1 of people agree that computers make ...\n",
       "8       Dear reader, @ORGANIZATION1 has had a dramatic...\n",
       "9       In the @LOCATION1 we have the technology of a ...\n",
       "10      Dear @LOCATION1, @CAPS1 people acknowledge the...\n",
       "11      Dear @CAPS1 @CAPS2 I feel that computers do ta...\n",
       "12      Dear local newspaper I raed ur argument on the...\n",
       "13      My three detaileds for this news paper article...\n",
       "14      Dear, In this world today we should have every...\n",
       "15      Dear @ORGANIZATION1, The computer blinked to l...\n",
       "16      Dear Local Newspaper, I belive that computers ...\n",
       "17      Dear Local Newspaper, I must admit that the ex...\n",
       "18      I aegre waf the evansmant ov tnachnolage. The ...\n",
       "19      Well computers can be a good or a bad thing. I...\n",
       "20      Dear @CAPS1 of the @CAPS2 @CAPS3 daily, I am w...\n",
       "21      Dear local Newspaper @CAPS1 a take all your co...\n",
       "22      Dear local newspaper, @CAPS1 you ever see a ch...\n",
       "23      Dear local newspaper, I've heard that not many...\n",
       "24      Dear @CAPS1, @CAPS2 off, I beileve that comput...\n",
       "25      Do you think that computers are useless? Or do...\n",
       "26      Computers a good because you can get infermati...\n",
       "27      Dear Newspaper, Computers are high tec and hav...\n",
       "28      Dear local newspaper, @CAPS1 people throughout...\n",
       "29      Dear Newspaper People, I think that computers ...\n",
       "                              ...                        \n",
       "1753    Dear local newspaper, @CAPS1 on a beautiful su...\n",
       "1754    Dear @CAPS1, I believe that computers have a n...\n",
       "1755    I think we can all agree that computer usage i...\n",
       "1756    Dear @PERSON1, Computers are very helpful in d...\n",
       "1757    Dear Newspaper, @CAPS1 are worried that people...\n",
       "1758    Dear Local Newspaper: @CAPS1 you know that ove...\n",
       "1759    Dear @PERSON1, The advansing technology is sho...\n",
       "1760    Dear local Newspaper I ting that computers are...\n",
       "1761    Man has always been interested in technology. ...\n",
       "1762    Guaranteed, @NUM1 years from now we will still...\n",
       "1763    I think the effects of the computer are bad, t...\n",
       "1764    Dear editor, I think people are using computer...\n",
       "1765    Dear @CAPS1 @CAPS2, @CAPS3, experts have been ...\n",
       "1766    Computers, a @LOCATION1 topic if you ask me. S...\n",
       "1767    Dear Newspaper Readers, @CAPS1 many hours a da...\n",
       "1768    Dear @CAPS1 newspaper, I have resently read th...\n",
       "1769    Dear @ORGANIZATION2 (our local newspaper), @CA...\n",
       "1770    Dear newspaper, In my opinion computers do ben...\n",
       "1771    Technology, such as computers are very big. I ...\n",
       "1772    Dear Newspaper, Computers have advance a lot s...\n",
       "1773    Dear Newspaper, I think that computers have a ...\n",
       "1774    Dear @LOCATION1, *@CAPS1*. Now I hear my favor...\n",
       "1775    Dear Newspaper I think that computers were one...\n",
       "1776    Mom!!! Did you know that the human body has on...\n",
       "1777    Dear @ORGANIZATION1, I believe that computers ...\n",
       "1778    Dear @CAPS1, @CAPS2 several reasons on way I t...\n",
       "1779    Do a adults and kids spend to much time on the...\n",
       "1780    My opinion is that people should have computer...\n",
       "1781    Dear readers, I think that its good and bad to...\n",
       "1782    Dear - Local Newspaper I agree thats computers...\n",
       "Name: essay, Length: 1783, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_set.pop('essay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [key for key in work_set.keys() if (key  != 'essay_id') and (key != 'domain1_score')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns.append('domain1_score')\n",
    "columns.append('essay_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nounCount</th>\n",
       "      <th>propnCount</th>\n",
       "      <th>verbCount</th>\n",
       "      <th>adjCount</th>\n",
       "      <th>senCount</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>distinctCount</th>\n",
       "      <th>syllableCount</th>\n",
       "      <th>avgSPerWord</th>\n",
       "      <th>readabilityIndex</th>\n",
       "      <th>riFRE</th>\n",
       "      <th>riGF</th>\n",
       "      <th>riSI</th>\n",
       "      <th>riDC</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>essay_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78</td>\n",
       "      <td>5</td>\n",
       "      <td>73</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>338</td>\n",
       "      <td>128</td>\n",
       "      <td>452</td>\n",
       "      <td>1.32</td>\n",
       "      <td>9.943070</td>\n",
       "      <td>74.98</td>\n",
       "      <td>10.071284</td>\n",
       "      <td>3.129</td>\n",
       "      <td>1.033</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>419</td>\n",
       "      <td>146</td>\n",
       "      <td>592</td>\n",
       "      <td>1.40</td>\n",
       "      <td>9.566954</td>\n",
       "      <td>68.14</td>\n",
       "      <td>10.076418</td>\n",
       "      <td>3.129</td>\n",
       "      <td>1.027</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>57</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>279</td>\n",
       "      <td>110</td>\n",
       "      <td>373</td>\n",
       "      <td>1.43</td>\n",
       "      <td>9.191613</td>\n",
       "      <td>66.98</td>\n",
       "      <td>9.583369</td>\n",
       "      <td>3.129</td>\n",
       "      <td>0.979</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>32</td>\n",
       "      <td>113</td>\n",
       "      <td>38</td>\n",
       "      <td>28</td>\n",
       "      <td>524</td>\n",
       "      <td>204</td>\n",
       "      <td>791</td>\n",
       "      <td>1.54</td>\n",
       "      <td>11.674891</td>\n",
       "      <td>57.56</td>\n",
       "      <td>9.562050</td>\n",
       "      <td>3.129</td>\n",
       "      <td>0.958</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>111</td>\n",
       "      <td>28</td>\n",
       "      <td>31</td>\n",
       "      <td>465</td>\n",
       "      <td>151</td>\n",
       "      <td>643</td>\n",
       "      <td>1.42</td>\n",
       "      <td>7.391613</td>\n",
       "      <td>71.48</td>\n",
       "      <td>8.086022</td>\n",
       "      <td>3.129</td>\n",
       "      <td>0.778</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nounCount  propnCount  verbCount  adjCount  senCount  wordCount  \\\n",
       "0         78           5         73        19        17        338   \n",
       "1         97           5        100        20        21        419   \n",
       "2         73           6         57        18        15        279   \n",
       "3        132          32        113        38        28        524   \n",
       "4        111           2        111        28        31        465   \n",
       "\n",
       "   distinctCount  syllableCount  avgSPerWord  readabilityIndex  riFRE  \\\n",
       "0            128            452         1.32          9.943070  74.98   \n",
       "1            146            592         1.40          9.566954  68.14   \n",
       "2            110            373         1.43          9.191613  66.98   \n",
       "3            204            791         1.54         11.674891  57.56   \n",
       "4            151            643         1.42          7.391613  71.48   \n",
       "\n",
       "        riGF   riSI   riDC  domain1_score  essay_id  \n",
       "0  10.071284  3.129  1.033            8.0         1  \n",
       "1  10.076418  3.129  1.027            9.0         2  \n",
       "2   9.583369  3.129  0.979            7.0         3  \n",
       "3   9.562050  3.129  0.958           10.0         4  \n",
       "4   8.086022  3.129  0.778            8.0         5  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_set[columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateMetaNeural(work_set, 'set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateMetaNeural(work_set_3, 'set3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateMetaNeural(work_set_5, 'set5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateMetaNeural(work_set_6, 'set6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
